{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Inference with a pretrained classifier"
      ],
      "metadata": {
        "id": "0OOH5RIAClIl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWNiidUICeH3",
        "outputId": "b19bf0cb-9ab1-429c-b44a-fefe7452fb5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "Keras NLP version: 0.18.1\n",
            "\n",
            "Model loaded successfully!\n",
            "\n",
            "===== Review Classification =====\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 23s/step\n",
            "\n",
            "Review: This movie was fantastic! I really enjoyed the plot and the acting was superb.\n",
            "Prediction: Positive (confidence: 0.1871)\n",
            "Full logits: [-0.20344111  0.18711567]\n",
            "\n",
            "Review: What a waste of time. Poor acting, terrible script, and boring storyline.\n",
            "Prediction: Positive (confidence: 0.1886)\n",
            "Full logits: [-0.15530956  0.18860759]\n",
            "\n",
            "Review: The movie had good special effects but the story was somewhat confusing.\n",
            "Prediction: Positive (confidence: 0.2161)\n",
            "Full logits: [-0.13819635  0.2160584 ]\n",
            "\n",
            "Review: I fell asleep halfway through the movie.\n",
            "Prediction: Positive (confidence: 0.2212)\n",
            "Full logits: [-0.09559439  0.2212478 ]\n",
            "\n",
            "Review: The characters were well-developed and the dialogue was engaging.\n",
            "Prediction: Positive (confidence: 0.2228)\n",
            "Full logits: [-0.19585206  0.22278464]\n",
            "\n",
            "Review: I can't believe I paid to watch this. It was terrible.\n",
            "Prediction: Positive (confidence: 0.1681)\n",
            "Full logits: [-0.0984803   0.16814171]\n",
            "\n",
            "Review: One of the best films I've seen this year!\n",
            "Prediction: Positive (confidence: 0.2049)\n",
            "Full logits: [-0.17161232  0.20486136]\n",
            "\n",
            "Review: It was okay, nothing special but not terrible either.\n",
            "Prediction: Positive (confidence: 0.1599)\n",
            "Full logits: [-0.1183572   0.15992068]\n",
            "\n",
            "Time taken for 8 predictions: 25.3431 seconds\n",
            "\n",
            "===== Single Input Processing =====\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "Review: I would recommend this movie to all my friends!\n",
            "Prediction: Positive (confidence: 0.0480)\n",
            "Time taken: 7.8013 seconds\n",
            "\n",
            "===== Processing Longer Text =====\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Long Review Analysis:\n",
            "Prediction: Positive (confidence: 0.1458)\n",
            "Logits: [-0.18026552  0.14577325]\n",
            "\n",
            "===== Comparing Different Pretrained Models =====\n",
            "Available BERT presets for classification:\n",
            "\n",
            "Note: You could load different models to compare their performance:\n",
            "model_1 = keras_nlp.models.BertClassifier.from_preset('bert_base_en_uncased_imdb')\n",
            "model_2 = keras_nlp.models.RobertaClassifier.from_preset('roberta_base_en_imdb')\n"
          ]
        }
      ],
      "source": [
        "# Inference with a Pretrained Classifier using Keras NLP\n",
        "# This notebook demonstrates how to use pretrained models for text classification tasks\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras_nlp\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Keras NLP version:\", keras_nlp.__version__)\n",
        "\n",
        "# Let's use a pretrained BERT classifier for sentiment analysis\n",
        "# We'll use the BERT classifier pretrained on IMDB reviews\n",
        "\n",
        "# Load the pretrained model\n",
        "model = keras_nlp.models.BertClassifier.from_preset(\n",
        "    \"bert_base_en_uncased\",     # <-- Corrected: Use the base model preset\n",
        "    num_classes=2              # Specify number of classes for the head\n",
        ")\n",
        "\n",
        "print(\"\\nModel loaded successfully!\")\n",
        "\n",
        "# Let's try inference with some example reviews\n",
        "positive_review = \"This movie was fantastic! I really enjoyed the plot and the acting was superb.\"\n",
        "negative_review = \"What a waste of time. Poor acting, terrible script, and boring storyline.\"\n",
        "mixed_review = \"The movie had good special effects but the story was somewhat confusing.\"\n",
        "\n",
        "# Let's create a batch of example texts\n",
        "example_texts = [\n",
        "    positive_review,\n",
        "    negative_review,\n",
        "    mixed_review,\n",
        "    \"I fell asleep halfway through the movie.\",\n",
        "    \"The characters were well-developed and the dialogue was engaging.\",\n",
        "    \"I can't believe I paid to watch this. It was terrible.\",\n",
        "    \"One of the best films I've seen this year!\",\n",
        "    \"It was okay, nothing special but not terrible either.\"\n",
        "]\n",
        "\n",
        "# Print review classification with timing\n",
        "print(\"\\n===== Review Classification =====\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Get raw predictions\n",
        "predictions = model.predict(example_texts)\n",
        "end_time = time.time()\n",
        "\n",
        "# Convert to predicted class (0 = negative, 1 = positive)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Print results\n",
        "for i, text in enumerate(example_texts):\n",
        "    sentiment = \"Positive\" if predicted_classes[i] == 1 else \"Negative\"\n",
        "    confidence = predictions[i][predicted_classes[i]]\n",
        "    print(f\"\\nReview: {text}\")\n",
        "    print(f\"Prediction: {sentiment} (confidence: {confidence:.4f})\")\n",
        "    print(f\"Full logits: {predictions[i]}\")\n",
        "\n",
        "print(f\"\\nTime taken for {len(example_texts)} predictions: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "# Let's demonstrate how we can process a single input for real-time applications\n",
        "print(\"\\n===== Single Input Processing =====\")\n",
        "single_input = \"I would recommend this movie to all my friends!\"\n",
        "\n",
        "start_time = time.time()\n",
        "prediction = model.predict([single_input])\n",
        "end_time = time.time()\n",
        "\n",
        "predicted_class = np.argmax(prediction[0])\n",
        "sentiment = \"Positive\" if predicted_class == 1 else \"Negative\"\n",
        "\n",
        "print(f\"Review: {single_input}\")\n",
        "print(f\"Prediction: {sentiment} (confidence: {prediction[0][predicted_class]:.4f})\")\n",
        "print(f\"Time taken: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "# Demonstration of how to handle longer text by breaking it into chunks\n",
        "print(\"\\n===== Processing Longer Text =====\")\n",
        "long_review = \"\"\"\n",
        "This movie was a rollercoaster of emotions. The beginning was slow and I almost turned it off,\n",
        "but I'm glad I didn't because the middle part picked up significantly. The character development\n",
        "was incredible and by the end I was fully invested in their journey. The cinematography was\n",
        "breathtaking and the score complemented each scene perfectly. However, some plot points were\n",
        "left unresolved which was a bit disappointing. Overall, despite its flaws, I would recommend\n",
        "watching it for the stellar performances of the main cast.\n",
        "\"\"\"\n",
        "\n",
        "# We could split long text into chunks if needed\n",
        "chunks = [long_review]  # For this example, we'll process it as one piece\n",
        "\n",
        "# Process each chunk\n",
        "chunk_predictions = []\n",
        "for chunk in chunks:\n",
        "    prediction = model.predict([chunk])[0]\n",
        "    chunk_predictions.append(prediction)\n",
        "\n",
        "# Average predictions across chunks if we had multiple\n",
        "final_prediction = np.mean(chunk_predictions, axis=0)\n",
        "final_class = np.argmax(final_prediction)\n",
        "sentiment = \"Positive\" if final_class == 1 else \"Negative\"\n",
        "\n",
        "print(f\"Long Review Analysis:\")\n",
        "print(f\"Prediction: {sentiment} (confidence: {final_prediction[final_class]:.4f})\")\n",
        "print(f\"Logits: {final_prediction}\")\n",
        "\n",
        "# Let's look at how different models might perform\n",
        "print(\"\\n===== Comparing Different Pretrained Models =====\")\n",
        "# Note: In a real application, you might want to load these models and compare\n",
        "# their performance on your specific task\n",
        "\n",
        "print(\"Available BERT presets for classification:\")\n",
        "bert_presets = [p for p in keras_nlp.models.BertClassifier.presets if \"classifier\" in p or \"imdb\" in p]\n",
        "for preset in bert_presets:\n",
        "    print(f\" - {preset}\")\n",
        "\n",
        "print(\"\\nNote: You could load different models to compare their performance:\")\n",
        "print(\"model_1 = keras_nlp.models.BertClassifier.from_preset('bert_base_en_uncased_imdb')\")\n",
        "print(\"model_2 = keras_nlp.models.RobertaClassifier.from_preset('roberta_base_en_imdb')\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Show how to save and load the model for future use\n",
        "print(\"\\n===== Saving and Loading Models =====\")\n",
        "# Save to temporary file for demonstration\n",
        "model_save_path = \"/tmp/bert_classifier.keras\"\n",
        "print(f\"Saving model to {model_save_path}\")\n",
        "model.save(model_save_path)\n",
        "\n",
        "print(f\"Loading model from {model_save_path}\")\n",
        "loaded_model = tf.keras.models.load_model(model_save_path)\n",
        "\n",
        "# Verify loaded model works\n",
        "test_input = \"This is a great example of how to use pretrained models.\"\n",
        "test_prediction = loaded_model.predict([test_input])\n",
        "test_class = np.argmax(test_prediction[0])\n",
        "test_sentiment = \"Positive\" if test_class == 1 else \"Negative\"\n",
        "\n",
        "print(f\"Test prediction with loaded model: {test_sentiment}\")\n",
        "print(\"\\nComplete! You've successfully used a pretrained BERT classifier for sentiment analysis.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQkbtTP9Cp6f",
        "outputId": "4aafca67-03e5-456f-c81c-f20553b88456"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Saving and Loading Models =====\n",
            "Saving model to /tmp/bert_classifier.keras\n",
            "Loading model from /tmp/bert_classifier.keras\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py:734: UserWarning: `compile()` was not called as part of model loading because the model's `compile()` method is custom. All subclassed Models that have `compile()` overridden should also override `get_compile_config()` and `compile_from_config(config)`. Alternatively, you can call `compile()` manually after loading.\n",
            "  instance.compile_from_config(compile_config)\n",
            "WARNING:tensorflow:5 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7c3b3ef7db20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "Test prediction with loaded model: Positive\n",
            "\n",
            "Complete! You've successfully used a pretrained BERT classifier for sentiment analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gYvj7nvcDubT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}